This is a document of the ideas that interested me as of July 13, 2020. I have
evolved my thinking a little bit since then, including the realisation that
other functional and dependently typed language ideas could be built off of
this core as well. Recently I have realised that something similar to this
design would be ideal for the maths libraries I would like to write, so I am
actually trying to implement the language again, with some modifications. For
now I will leave this document here, to be corrected later.

High level structure
====================

Two-pass language, one pass typechecks and generates RISC bytecode, the next
pass compiles to the backend, using information from the first pass to allocate
registers and stack memory heuristically. The second pass will also perform the
usual const propagation and strength weakening

collections of bytecode will be called algorithms, algorithms can be freely
inlined into other algorithms, or executed at compile time, or inlined in
higher-order closure type algorithms.

types will also be first-class objects that can be generated by bytecode
algorithms at compile time, allowing for parameterized type families, and
associated generic/dependently typed programming that can come from that.

Ideally linking would be a light-weight process, where holes are recorded in a
table, and filled through blind pointer writes

control flow and arithmetic operators could be short algorithms that get
inlined, all user-defined in that way (inspired by a language called AYY)

types will dictate whether certain operations are allowed, and how those
operations will translate into bytecode, but once bytecode exists it is treated
as a black box operating blindly on binary/`void*` data.

algorithms cannot be called, only inlined, they must be compiled into
procedures that can be called from other algorithms.

Algorithm bindings
==================

Algorithms have a couple of ways of declaring variables, which each have
different semantics, the most fundamental of which is the `var` keyword:
```
var x: TYPE;
var y = EXPR;
x = EXPR;
```
This declares a new variable analogous to a register. These variables cannot be
addressed, but can be modified dynamically. EXPR is an expression, which
essentially means an arbitrary algorithm, which will be inlined, and ultimately
returned

Next is an SSA style immutable declaration:
`x := EXPR;`
this evaluates an expression (an algorithm) and binds the result to a variable
named `x`, which cannot be assigned to after that. In implementation this is
equivalent to `var`, but provides protection against being overwritten as a
convenience. These immutable values will be exceptionally useful in any
experiments with propositions, should they be explored in the future.

Finally local and global memory declarations:
```
local x: TYPE;
local y = EXPR;
*x = EXPR;

global z: TYPE;
```
these will be locations in the stack or data segment, whose addresses will be
available as pointers `*TYPE` and that can be written to by dereferencing on
the LHS of an assignment. This does not produce an lvalue, merely says that the
address should be written to rather than overwriting the address itself -
something that cannot be done since local bindings are also immutable.

All of these concepts are specific to the bytecode, when compiled in the second
pass they will all be reduced to concrete registers and stack offsets.
Additionally the bytecode will store the next read, and last read, of each
`var` that is written to, allowing the second pass to heuristically prioritise
vars that have the longest time before their next read, in situations where a
var must be stored on the stack to make room in the corresponding register.

Novel Pointer semantics
=======================

There are two pointer types in this language, `*T` and `*alias T`, similar to
the rust types `&mut` and `*mut` respectively, in that the former is unique,
and the latter aliased. Aliased pointers can be modified, and are much the same
as pointers in C, but unique pointers are special in that their type can change
from line to line.

```
local x: f64 = 0.0;
*x: u64 = 5; // x is now a *u64
```

Algorithms and procedures can modify the type of any inputs that are unique
pointers, allowing for a clear semantic distinction between initialization,
modification, and destruction of data structures.

When accessing fields or indeces of a pointer-to-struct or pointer-to-array,
a pointer to the corresponding field is returned.

```
local pair: struct{x: u32, y: u32};
var yptr: *u32 = pair.x;
```

Field access has higher syntactic precedence than dereferencing, as in C, so
expressions like `*thing.field` are available and equivalent to the
`thing->field` one would use in C.

Unique pointers can be reborrowed as in Rust, which can be used to modify the
types of structs:

```
local pair: struct{x: u32, y: u32};
{
	var xptr: *u32 = pair.x;
	*xptr: f32 = 0.0;
}
// pair is now a struct{x: f32, y: u32}
```

Various no-op reinterpretations of structs are also possible using `:` syntax
or `as` syntax.

```
local triple: struct{x: u32, y: u32, z: u32};
triple: *struct{struct{x: u32, y: u32}, n: u32};
f(triple as *struct{x: u32, struct{y: u32, z: u32}});
```

One-way reinterpretations of this form are also possible, such as dropping
fields from the end of a struct, or dropping the compound altogether to point
to the first field without using any field access.

Pointers to a variable number of elements have the type `*[T]`, like a hybrid
between a plain `*T` in C and a flexible array member in C99. Any pointer can
be implicitly cast to pointer-to-flexible-array of the same type, or of the
first field of that type.

Additionally zero-length-arrays can be thought of existing at any location with
appropriate alignment, and so pointers to flexible arrays can be explicitly
cast to any other pointer to flexible array with weaker alignment constraints.

Pointer arithmetic can only be done on pointer-to-flexible-array, and will
implicitly cast other pointer types into such a pointer first.

Offsetting to the latter half of a pointer-to-struct can be done with the @
operator:
```
local triple: struct{x: u32, y: u32, z: u32};
var yz: *struct{y: u32, z: u32} = triple@y;
```
this can be useful when a sequence of fields ought to be available as a
sub-structure, which in Jai could be done by actually writing a substructure
and then using the `using` keyword.

Pointers can point partway through a data structure, giving some fields a
negative offset, which can allow for more flexible pointer reinterpretations,
such as the stb.h style array structure:
```
local pair: struct{len: u32, data: [u32]};
var data: *struct{len: u32, @data: [u32]} = pair@data;
if (*data.len > 3) *data[3] = 22;
assert(data.len as *[u32] - data as *[u32] == -4);
```

Polymorphic Interfaces
======================

Many procedures in low level languages can be used polymorphically without
compiling multiple implementations, but in C must typically use `void*` in
order to type-check.

For example consider the following two procedures:
```
proc f(x: *u32, y: u32) { *x = y; }
proc g(x: *u32, y: f32) -> (x: *f32) { *x = y; }
```
these procedures have the same implementation, write `y` to the location at
`x`. Such a redundancy could be found automatically by an advanced linker, but
this is a two-pass language that shouldn't need this complexity, and further
having to write code that isn't even written to the binary twice can become
cumbersome.

For these reasons it would be nice to write functions once, but declare
multiple interfaces for them, or perhaps define families of interface that can
all be used:
```
f: proc(x: *u32, y: u32);
f: proc (x: *u32, y: f32) -> (x: *f32);
f: forall A: Type, B: Type, A < b32, B < b32 ->
  proc (x: *A, y: B) -> (x: *B);
f: forall A: Type, A < b32 ->
  proc (x: *alias A, y: A)
f(x, y) { *x = y; }
```

Metaprogramming
===============

During compilation, all algorithms and procedures become bytecode that can be
run at compile time, or inlined into other algorithms. This means that
algorithms can be defined from within other algorithms, in a manner resembling
closures from many dynamic languages:
```
alg f(x: u32) -> alg(y: u32) -> u32 {
	alg g(y: u32) -> u32 {
		return x + y;
	}
	return g;
}
```
The above algorithm `f` would become a sequence of bytecode operations that
would take an input var `x`, and call compiler functions to generate the
bytecode appropriate for the function `g`, which would then be returned.

This can be used to achieve polymorphism explicitly using first-class types:
```
alg make_proc(A: Type) -> proc(x: A) -> A {
	proc p(x: A) -> A { return x; }
	return p;
}
```

Combining this with default arguments and operator overloading can create
something analogous to abstract classes/traits/interfaces/duck typing:
```
alg make_sum(A: Type, f: (alg(A, A) -> A) = add, default: A = 0)
	-> proc(x: struct {len: u32, data: *[A]}) -> A
{
	proc sum(x: struct {len: u32, data: *[A]}) -> A {
		var i = 0;
		local out: A = default;
		while (i < x.len) {
			*out = f(*out, *x.data[i]);
			i = i + 1;
		}
	}
	return sum;
}
```

These closure algorithms can be made more succinct, or simply more unreadable,
by using a one-liner syntax:
```
alg f(A: Type) := proc(x: A) -> A { return x; };
alg g(A: Type) := proc(x: A) := x;
```

New Metaprogramming
===================

Iterating on the above ideas, my current design now looks like this:

- Functions `func` are inlinable bytecode exactly like the `alg`s from above.
- Procedures `proc` are inlinable bytecode that will also be compiled into
  linkable machine code in the second pass, just as above.
- Functions can be declared `static` to prevent them from being inlined into
  procedures, only other static functions.
- Macros `macro` are static functions that emit bytecode to be included in a
  `func` or `proc`.

Then inside a `func` or `proc` one can have `static` and `macro` blocks which
are allowed to call `static func`s and `macro`s, but can only use static
values. All locally scoped variables become static `ident` identifiers, which
can be manipulated through compiler/interpretor APIs/builtins. A static
function implicitly has a static block for its body, and similar for macros.

Inside `macro` blocks, and maybe after initialising a new function declaration
in a `static` block, one can access `emit_bytecode` builtins which add
operations to the relevant bytecode body. This is made easier by the `emit`
keyword which when applied to blocks or statements will essentially desugar to
a series of `emit_bytecode` operations. The `emit_bytecode` operation will have
a corresponding bytecode representation, so that macros can be generated in the
first pass and run. `macro` blocks, then, are regions of bytecode that will be
run upon inlining, in order to generate the actual bytecode to be compiled into
machine code. Unclear whether macro blocks will need to be expanded, and hence
whether functions will need to be inlined, in the first pass or in the second
pass.

A `body` keyword might help for defining `func`s using a single macro
expansion, although `func f() macro {...}` might have the same effect.

Importantly `macro` and `emit` can be chained on single statements for things
like loop unrolling:
```
func f(static n: u32, xs: *[u32 * n]) {
    macro for i in n emit {
        xs[i] *= 2;
    }
}
```

